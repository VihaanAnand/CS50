# Specification
- The `get_model` function should return a compiled neural network model.
  - You may assume that the input to the neural network will be of the shape `(IMG_WIDTH, IMG_HEIGHT, 3)` (that is, an array representing an image of width `IMG_WIDTH`, height `IMG_HEIGHT`, and `3` values for each pixel for red, green, and blue).
  - The output layer of the neural network should have `NUM_CATEGORIES` units, one for each of the traffic sign categories.
  - The number of layers and the types of layers you include in between are up to you. You may wish to experiment with:
    - different numbers of convolutional and pooling layers
    - different numbers and sizes of filters for convolutional layers
    - different pool sizes for pooling layers
    - different numbers and sizes of hidden layers
    - dropout

# Experimentation
**Goal:** Accuracy above 95.76%, time under 16 ms/step
| Model Description | Change | Time | Accuracy | Results |
| ----------------- | ------ | ---- | -------- | ------- |
| 1) Convolution - 32 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Flattening<br>4) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>5) Output layer - Softmax activation | Starting code from lecture, `handwriting.py` | 5.30% | 13 ms/step | Decent speed but extremely inaccurate |
| 1) Convolution - 32 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 32 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Added one more convolution layer after the max pooling | 95.88% | 16 ms/step | A little slower, but very high accuracy |
| 1) Convolution - 32 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 32 times, 3x3, ReLU activation<br>4) Max pooling - 2x2<br>5) Flattening<br>6) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>7) Output layer - Softmax activation | Added one more max pooling layer after the 2nd convolution | 93.76% | 14 ms/step | Slightly faster and slightly less accurate |
| 1) Convolution - 64 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 64 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Removed the 2nd max pooling layer, changed the convolution to 64 times each | 96.65% | 34 ms/step | Extremely slow model, but the accuracy didn't change much from trial 2 |
| 1) Convolution - ३२ times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 64 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Changed the first convolution layer to 32 times | 95.73% | २4 ms/step | Somewhat slow model, but the accuracy didn't change much from trial 4 |
| 1) Convolution - 48 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 48 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Changed both convolution layers to 48 times each | 97.43% | २3 ms/step | Somewhat slow model, with extremely high accuracy |
| 1) Convolution - 48 times, 5x5, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 48 times, 5x5, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Changed both convolution layers to 5x5 each | 95.81% | 30 ms/step | Very slow model, with a noticable decrease in accuracy |
| 1) Convolution - 48 times, 3x3, ReLU activation<br>2) Max pooling - 4x4<br>3) Convolution - 48 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Changed both convolution layers to 3x3 each, and changed max pooling to 4x4 | 95.81% | 30 ms/step | Very slow model, with a noticable decrease in accuracy |
| 1) Convolution - 48 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 48 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 128 neurons, ReLU activation<br>6) Hidden layer - 128 neurons, 50% dropout, ReLU activation<br>7) Output layer - Softmax activation | Changed the max pooling layer to 2x2, added a hidden layer with 128 neurons and ReLU activation | 96.40% | 23 ms/step | Somewhat slow model, very high accuracy |
| 1) Convolution - 48 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 48 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 256 neurons, ReLU activation<br>6) Hidden layer - 256 neurons, 50% dropout, ReLU activation<br>7) Output layer - Softmax activation | Changed both hidden layers to 256 neurons | 95.55% | 29 ms/step | Decently slow model, very high accuracy |
| 1) Convolution - 48 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 48 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 256 neurons, 50% dropout, ReLU activation<br>6) Output layer - Softmax activation | Removed first hidden layer | 96.97% | 28 ms/step | Somewhat slow model, with very high accuracy |
| 1) Convolution - 48 times, 3x3, ReLU activation<br>2) Max pooling - 2x2<br>3) Convolution - 48 times, 3x3, ReLU activation<br>4) Flattening<br>5) Hidden layer - 256 neurons, 25% dropout, ReLU activation<br>6) Output layer - Softmax activation | Changed dropout to 25% | 95.68% | 28 ms/step | Somewhat slow model, with very high accuracy |

## Paragraph Form
The goal of the experimentation was to achieve an accuracy above 95.76% while maintaining a processing time under 16 ms per step. The initial model, derived from a lecture example, was extremely fast but highly inaccurate, prompting iterative modifications. Adding a second convolutional layer significantly improved accuracy to 95.88% but slightly increased processing time. Adjustments to convolutional filters, pooling sizes, and hidden layers yielded mixed results. A standout configuration included two convolutional layers (48 filters each, 3×3 kernels, ReLU activation), max pooling (2×2), a dense layer with 128 neurons, 50% dropout, and softmax output, achieving an accuracy of 97.43% at 23 ms per step, balancing high accuracy with reasonable speed.
